<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SWE-QA: A Dataset and Benchmark for Complex Code Understanding</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Roboto:wght@300;400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1>SWE-QA: A Dataset and Benchmark for Complex Code Understanding</h1>
            <p class="subtitle">Anonymous submission for LREC 2026</p>
        </header>

        <section class="teaser">
            <object data="assets/teaser.pdf" type="application/pdf" class="teaser-pdf">
                <p>Your browser doesn't support embedded PDFs. <a href="assets/teaser.pdf">Download the teaser figure</a> instead.</p>
            </object>
        </section>

        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                We present SWE-QA, a comprehensive dataset and benchmark for evaluating multi-hop code comprehension. 
                The dataset contains <strong>9,072 multiple-choice questions</strong> generated from <strong>12 Python repositories</strong> 
                from SWE-bench, designed to test complex code understanding capabilities across real-world software projects.
            </p>
        </section>

        <section class="paper">
            <h2>Paper</h2>
            <div class="paper-link">
                <a href="paper/paper.pdf" class="btn btn-primary" download>
                    <span class="icon">üìÑ</span>
                    Download Paper (PDF)
                </a>
            </div>
        </section>

        <section class="dataset">
            <h2>Dataset</h2>
            <p>
                The SWE-QA dataset is designed to evaluate multi-hop code comprehension with two question categories 
                and two experimental settings:
            </p>
            
            <div class="dataset-features">
                <div class="feature-card">
                    <h3>9,072 Questions</h3>
                    <p>Multiple-choice questions for code comprehension</p>
                </div>
                <div class="feature-card">
                    <h3>12 Repositories</h3>
                    <p>From popular Python projects in SWE-bench</p>
                </div>
                <div class="feature-card">
                    <h3>2 Categories</h3>
                    <p>Declaration-and-Call (DC) & Interacting Entities (IE)</p>
                </div>
                <div class="feature-card">
                    <h3>2 Settings</h3>
                    <p>Oracle (clean) & Noisy Oracle (with distractors)</p>
                </div>
            </div>

            <div class="dataset-details">
                <h3>Dataset Structure</h3>
                <ul>
                    <li><strong>mcq_dataset.zip</strong> - Oracle setting questions (9,072 questions)</li>
                    <li><strong>mcq_dataset_with_distractors.zip</strong> - Noisy oracle setting questions</li>
                    <li><strong>repos/</strong> - Processed code chunks and entity metadata for 12 repositories</li>
                </ul>
                
                <h3>Repositories</h3>
                <div class="repo-list">
                    <span class="repo-badge">astropy</span>
                    <span class="repo-badge">django</span>
                    <span class="repo-badge">matplotlib</span>
                    <span class="repo-badge">seaborn</span>
                    <span class="repo-badge">flask</span>
                    <span class="repo-badge">requests</span>
                    <span class="repo-badge">pylint</span>
                    <span class="repo-badge">pytest</span>
                    <span class="repo-badge">xarray</span>
                    <span class="repo-badge">scikit-learn</span>
                    <span class="repo-badge">sphinx</span>
                    <span class="repo-badge">sympy</span>
                </div>
            </div>

            <div class="download-section">
                <a href="datasets/mcq_dataset.zip" class="btn btn-secondary" download>
                    <span class="icon">üì¶</span>
                    Oracle Questions
                </a>
                <a href="datasets/mcq_dataset_with_distractors.zip" class="btn btn-secondary" download>
                    <span class="icon">üì¶</span>
                    Noisy Oracle Questions
                </a>
                <a href="datasets/" class="btn btn-tertiary">
                    <span class="icon">üìÅ</span>
                    Browse All Datasets
                </a>
            </div>

            <div class="readme-section">
                <h3>Dataset Documentation</h3>
                <p>For detailed information about the dataset structure, JSON schemas, and usage instructions, 
                   please refer to the <a href="datasets/README.md">Dataset README</a>.</p>
            </div>
        </section>

        <section class="citation">
            <h2>Citation</h2>
            <p>If you use this dataset in your research, please cite our paper:</p>
            <pre><code>@inproceedings{swe-qa-2026,
  title={Multi-hop Code Comprehension: A Dataset and Benchmark for Complex Code Understanding},
  author={Anonymous},
  booktitle={Proceedings of LREC 2026},
  year={2026}
}</code></pre>
        </section>

        <footer>
            <p>¬© 2026 Anonymous Authors. For review purposes only.</p>
        </footer>
    </div>
</body>
</html>
